{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.auto import trange, tqdm\n",
    "from features import get_transformer, merge_entries\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from utills import chunker\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import init_notebook_mode\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/large/'\n",
    "GROUND_TRUTH_PATH = DATA_DIR + 'pan20-authorship-verification-training-large-truth.jsonl'\n",
    "DATA_PATH = DATA_DIR + 'pan20-authorship-verification-training-large.jsonl'\n",
    "TEMP_DATA_PATH = 'temp_data/large_model_training_data/'\n",
    "PREPROCESSED_DATA_PATH = 'temp_data/large_model_training_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_transformers(data_fraction=0.01):\n",
    "    docs_1 = []\n",
    "    docs_2 = []\n",
    "\n",
    "    with open(PREPROCESSED_DATA_PATH + 'preprocessed_train.jsonl', 'r') as f:\n",
    "        for l in tqdm(f):\n",
    "            if np.random.rand() < data_fraction:\n",
    "                d = json.loads(l)\n",
    "                docs_1.append(d['pair'][0])\n",
    "                docs_2.append(d['pair'][1])\n",
    "                \n",
    "    transformer = get_transformer()\n",
    "    scaler = StandardScaler()\n",
    "    secondary_scaler = StandardScaler()\n",
    "\n",
    "    X = transformer.fit_transform(docs_1 + docs_2).todense()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X1 = X[:len(docs_1)]\n",
    "    X2 = X[len(docs_1):]\n",
    "    secondary_scaler.fit(np.abs(X1 - X2))\n",
    "    \n",
    "    return transformer, scaler, secondary_scaler\n",
    "\n",
    "\n",
    "def vectorize(XX, Y, ordered_idxs, transformer, scaler, secondary_scaler, preprocessed_path, vector_Sz):\n",
    "    with open(preprocessed_path, 'r') as f:\n",
    "        batch_size = 10000\n",
    "        i = 0;\n",
    "        docs1 = []\n",
    "        docs2 = []\n",
    "        idxs = []\n",
    "        labels = []\n",
    "        for l in tqdm(f, total=vector_Sz):\n",
    "            d = json.loads(l)\n",
    "            \n",
    "            docs1.append(d['pair'][0])\n",
    "            docs2.append(d['pair'][1])\n",
    "\n",
    "            labels.append(ground_truth[d['id']])\n",
    "            idxs.append(ordered_idxs[i])\n",
    "            i += 1\n",
    "            if len(labels) >= batch_size:\n",
    "                x1 = scaler.transform(transformer.transform(docs1).todense())\n",
    "                x2 = scaler.transform(transformer.transform(docs2).todense())\n",
    "                XX[idxs, :] = secondary_scaler.transform(np.abs(x1-x2))\n",
    "                Y[idxs] = labels\n",
    "\n",
    "                docs1 = []\n",
    "                docs2 = []\n",
    "                idxs = []\n",
    "                labels = []\n",
    "\n",
    "        x1 = scaler.transform(transformer.transform(docs1).todense())\n",
    "        x2 = scaler.transform(transformer.transform(docs2).todense())\n",
    "        XX[idxs, :] = secondary_scaler.transform(np.abs(x1-x2))\n",
    "        Y[idxs] = labels\n",
    "        XX.flush()\n",
    "        Y.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth = {}\n",
    "with open(GROUND_TRUTH_PATH, 'r') as f:\n",
    "    for l in f:\n",
    "        d = json.loads(l)\n",
    "        ground_truth[d['id']] = d['same']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Sz: 206501\n",
      "Test Sz: 69064\n"
     ]
    }
   ],
   "source": [
    "train_sz = 206501\n",
    "test_sz = 69064\n",
    "\n",
    "# with open(PREPROCESSED_DATA_PATH + 'preprocessed_train.jsonl', 'r') as f:\n",
    "#     for l in f:\n",
    "#         train_sz += 1\n",
    "\n",
    "# with open(PREPROCESSED_DATA_PATH + 'preprocessed_test.jsonl', 'r') as f:\n",
    "#     for l in f:\n",
    "#         test_sz += 1\n",
    "\n",
    "print('Train Sz:', train_sz, flush=True)\n",
    "print('Test Sz:', test_sz, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fitting transformer...', flush=True)\n",
    "transformer, scaler, secondary_scaler = fit_transformers(data_fraction=0.05)\n",
    "feature_sz = len(transformer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vectorizing train set...', flush=True)\n",
    "XX_train = np.memmap(TEMP_DATA_PATH + 'vectorized_XX_train.npy', dtype='float32', mode='w+', shape=(train_sz, feature_sz))\n",
    "Y_train = np.memmap(TEMP_DATA_PATH + 'Y_train.npy', dtype='int32', mode='w+', shape=(train_sz))\n",
    "train_idxs = np.array(range(train_sz))\n",
    "np.random.shuffle(train_idxs)\n",
    "\n",
    "vectorize(\n",
    "    XX_train, \n",
    "    Y_train, \n",
    "    train_idxs, \n",
    "    transformer, \n",
    "    scaler, \n",
    "    secondary_scaler, \n",
    "    PREPROCESSED_DATA_PATH + 'preprocessed_train.jsonl',\n",
    "    train_sz\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vectorizing test set...', flush=True)\n",
    "XX_test = np.memmap(TEMP_DATA_PATH + 'vectorized_XX_test.npy', dtype='float32', mode='w+', shape=(test_sz, feature_sz))\n",
    "Y_test = np.memmap(TEMP_DATA_PATH + 'Y_test.npy', dtype='int32', mode='w+', shape=(test_sz))\n",
    "test_idxs = np.array(range(test_sz))\n",
    "np.random.shuffle(test_idxs)\n",
    "\n",
    "vectorize(\n",
    "    XX_test, \n",
    "    Y_test, \n",
    "    test_idxs, \n",
    "    transformer, \n",
    "    scaler, \n",
    "    secondary_scaler, \n",
    "    PREPROCESSED_DATA_PATH + 'preprocessed_test.jsonl',\n",
    "    test_sz\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Tuning parameters...', flush=True)\n",
    "\n",
    "\n",
    "param_dist = {'alpha': loguniform(1e-4, 1e0)}\n",
    "batch_size=100\n",
    "clf = SGDClassifier(loss='log', alpha=0.01)\n",
    "n_iter_search = 2\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, verbose=2)\n",
    "for idxs in chunker(range(train_sz), batch_size):\n",
    "        random_search.fit(XX_train[idxs, :], Y_train[idxs])\n",
    "        break\n",
    "\n",
    "print('Best params:', random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training classifier...', flush=True)\n",
    "clf = SGDClassifier(loss='log', alpha=random_search.best_params_['alpha'])\n",
    "batch_size=50000\n",
    "num_epochs = 50\n",
    "aucs = []\n",
    "for i in trange(num_epochs):\n",
    "    print('Epoch - ', i)\n",
    "    print('-' * 30)\n",
    "    for idxs in chunker(range(train_sz), batch_size):\n",
    "        clf.partial_fit(XX_train[idxs, :], Y_train[idxs], classes=[0, 1])\n",
    "\n",
    "    probs = clf.predict_proba(XX_test)[:, 1]\n",
    "    fpr, tpr, thresh = roc_curve(Y_test, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    print('AUC: ', roc_auc)\n",
    "    with open(TEMP_DATA_PATH + 'experiment_data.p', 'wb') as f:\n",
    "        pickle.dump((\n",
    "            aucs,\n",
    "            clf,\n",
    "            roc_auc,\n",
    "            transformer, \n",
    "            scaler,\n",
    "            secondary_scaler,\n",
    "            feature_sz,\n",
    "            train_sz,\n",
    "            train_idxs,\n",
    "            test_sz,\n",
    "            test_idxs\n",
    "        ), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go.Figure(go.Scatter(\n",
    "    x=np.arange(len(aucs)),\n",
    "    y=aucs\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(TEMP_DATA_PATH + 'large_model.p', 'wb') as f:\n",
    "    pickle.dump((clf, transformer, scaler, secondary_scaler), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.967, 'c@1': 0.909, 'f_05_u': 0.918, 'F1': 0.915, 'brier': 0.928, 'overall': 0.927}\n"
     ]
    }
   ],
   "source": [
    "from pan20_verif_evaluator import evaluate_all\n",
    "results = evaluate_all(Y_test, probs)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
